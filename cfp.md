# Important Dates 

- Paper submission deadline: <s>20 May 2022</s> 27 May 2022 (11:59 pm UTC-11).
- Acceptance notification: 13 June 2022.
- Camera-ready deadline: 30 June 2022 (11:59 pm UTC-11).
- Workshop: 23 Jully 2022.

# Author Instructions

The authors have to follow the instructions listed below when submitting their contributions:

- Use the ICML LaTeX [template](https://media.icml.cc/Conferences/ICML2022/Styles/icml2022.zip) to prepare the manuscript.
- Papers should not be longer than 4 pages, including figures and tables, and excluding references and appendix.
- Do not report author names since the review is double-blind.
- Papers that have been accepted in the main ICML conference cannot be submitted to the workshop.  
- Manuscripts should be submitted using the [CMT](https://cmt3.research.microsoft.com/HAET2022) system. 

We encourage all authors to also submit their work to arXiv. The workshop will have proceedings with copyright attributed to the authors. However, the inclusion of accepted papers in the workshop proceedings is optional.

# Scope and Topics

The workshop welcomes contributions that seek to reduce the cost of the training process based on high-level complexity metrics (number of operations, memory usage) or by taking into account the details of the computing hardware (CPU, GPU, IPU, NPU, or any other custom accelerator implemented as an ASIC or on FPGA). Topics of interest include (but are not limited to):

- compression methods to reduce memory usage and/or complexity of deep learning during training,
- hardware architectures and implementations for deep learning training,
- energy reduction techniques for deep learning training,
- open-source designs, implementations, and code related to efficient deep-learning implementations,
- energy models or energy-efficiency benchmarks for deep learning training implementations,
- applications of low-energy deep learning training,
- equilibrium-propagation-based techniques and/or their hardware implementations,
- few-shot/few-labels and semi-supervised learning methods for training on-chip,  
- other related contributions are welcome such as compression techniques for inference with deep learning architectures.
 
# Awards and Prizes

Thanks to our sponsors, in addition to best paper awards we will also award a prize for the most energy-efficient hardware architecture and a prize for the fastest training method on compute clusters (details coming soon).
